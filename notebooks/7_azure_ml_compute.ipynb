{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c53814",
   "metadata": {},
   "source": [
    "# 7. Provisioning an Azure ML Compute cluster  \n",
    "\n",
    "To run training jobs in the cloud we need an **AMLCompute** target.  \n",
    "This notebook cell will either **reuse** an existing cluster or **create** a new one with autoscaling rules.\n",
    "\n",
    "**Key concepts**\n",
    "\n",
    "| Concept | Why it matters | Default in this demo |\n",
    "|---------|----------------|----------------------|\n",
    "| **VM size (`size`)** | Determines CPU/GPU type, RAM & disk. | `\"STANDARD_D2_V3\"` (CPU) — change to e.g. `\"Standard_NC6s_v3\"` for a single Tesla V100. |\n",
    "| **Autoscaling** | Saves cost by shrinking to `min_instances` when idle, up to `max_instances` on demand. | `min=0`, `max=4`, scale-down after **3 min** idle. |\n",
    "| **Tier** | `Dedicated` (pay-as-you-go) or `LowPriority` (pre-emptible, cheaper). | `Dedicated`. |\n",
    "| **Idempotent creation** | Running the cell twice will detect the cluster and skip re-creation. | Handled in the helper. |\n",
    "\n",
    "> **Quota check**  \n",
    "> Ensure your subscription has quota for the chosen VM size in the selected region.  \n",
    "> If not, request a quota increase in the Azure portal before running this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffabbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "from azure.ai.ml.entities import AmlCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ddb17",
   "metadata": {},
   "source": [
    "## 1) Load settings from config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153db1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the YAML file\n",
    "with open(\"../config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089d276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = config[\"azure\"][\"subscription_id\"]\n",
    "resource_group_name = config[\"azure\"][\"resource_group_name\"]\n",
    "workspace_name = config[\"azure\"][\"workspace_name\"]\n",
    "\n",
    "training_gpu_cluster = config[\"azure\"][\"training_gpu_cluster\"]\n",
    "compute_name = config[\"azure\"][\"compute_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32684ec2",
   "metadata": {},
   "source": [
    "## 2) Authenticate & connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "385dcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae8ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "# Update Azure ML Client\n",
    "ml_client = MLClient(credential, subscription_id, resource_group_name, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1eb73",
   "metadata": {},
   "source": [
    "## 3) Create or verify Compute Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c635ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compute_resource(ml_client, cluster_name, size=\"STANDARD_D2_V3\", \n",
    "                            min_instances=0, max_instances=4, \n",
    "                            idle_time_before_scale_down=180, tier=\"Dedicated\"):\n",
    "    \"\"\"\n",
    "    Creates or reuses an AMLCompute resource in Azure ML.\n",
    "\n",
    "    Parameters:\n",
    "        ml_client (MLClient): The Azure ML client.\n",
    "        cluster_name (str): The name of the compute cluster.\n",
    "        size (str): The VM size (default is \"STANDARD_D2_V3\").\n",
    "        min_instances (int): The minimum number of instances (default is 0).\n",
    "        max_instances (int): The maximum number of instances (default is 4).\n",
    "        idle_time_before_scale_down (int): Idle time in seconds before scaling down (default is 180).\n",
    "        tier (str): The pricing tier, either \"Dedicated\" or \"LowPriority\" (default is \"Dedicated\").\n",
    "\n",
    "    Returns:\n",
    "        An instance of AmlCompute representing the compute resource.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the compute cluster already exists\n",
    "        compute_resource = ml_client.compute.get(cluster_name)\n",
    "        print(f\"A cluster named '{cluster_name}' already exists; reusing it.\")\n",
    "    except Exception:\n",
    "        print(\"Creating a new GPU compute resource...\")\n",
    "        compute_resource = AmlCompute(\n",
    "            name=cluster_name,\n",
    "            type=\"amlcompute\",\n",
    "            size=size,\n",
    "            min_instances=min_instances,\n",
    "            max_instances=max_instances,\n",
    "            idle_time_before_scale_down=idle_time_before_scale_down,\n",
    "            tier=tier,\n",
    "        )\n",
    "        # Create the compute resource and wait until the operation completes\n",
    "        compute_resource = ml_client.begin_create_or_update(compute_resource).result()\n",
    "    \n",
    "    print(f\"AMLCompute resource '{compute_resource.name}' is created with size '{compute_resource.size}'.\")\n",
    "    return compute_resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_compute_resource(ml_client, \n",
    "                        training_gpu_cluster,\n",
    "                        compute_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd582bc",
   "metadata": {},
   "source": [
    "### What’s next?\n",
    "\n",
    "* Subsequent training jobs can reference this cluster via  \n",
    "  `compute=azureml:${{training_gpu_cluster}}`.  \n",
    "* If you need **separate CPU and GPU pools**, repeat the helper with a different\n",
    "  `cluster_name` and `size`.\n",
    "* Cluster metadata (state, quota usage) is visible in **Azure ML Studio → Manage → Compute**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo11-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
